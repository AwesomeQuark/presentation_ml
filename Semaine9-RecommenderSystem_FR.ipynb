{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les systèmes de recommandation\n",
    "\n",
    "\n",
    "Les systèmes de recommandation visent à maximiser la consommation en proposant à l'utilisateur ou consommateur les éléments le plus susceptibles de l'intéresser.\n",
    "Ces systèmes se basent sur caractéristiques des utilisateurs qui se ressemblent pour sa \"prédiction\", on distingue trois méthodes de comparaison pour ce type d'algorithmes :\n",
    "\n",
    "    => Basé sur l'objet lui même, ou \"content-based approach\"\n",
    "    => Basé sur la personne\n",
    "    => Basé sur la société qui l'entoure et ses influences, ou \"Collaborative filtering, context aware\"\n",
    "    => Ou bien basé sur un melange de ces trois donnée, appellé recommandation hybride\n",
    "    \n",
    "Aujourd'hui, les content based approch et les collaborative filtering !\n",
    "Nous allons étudier des données de films, et leur évaluation par des utilisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on commence a connaitre !\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse des datas\n",
    "Chargeons les données depuis le fichier 'ml-100k/u.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creez une liste de string user_id, item_id, rating et timestamp qui serviront de noms au colonnes\n",
    "names = []\n",
    "# Lisez le fichier [pd.read_csv]\n",
    "data =\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous voyons, nous avons une colonne pour les users, une pour les films (item_id), une pour levaluation, et la dernière dont nous ne nous occuperons pas, désigme le moment auquel le film a été visionné en temps universel.<br>\n",
    "Funfact : les gagnant du netflix_prize ont pimpés leur algorithme notamment avec un parametre deduisant le biais temporel.\n",
    "<br>\n",
    "\n",
    "Jetons un coup d'oeil au nombre d'items et d'users pour verifier l'integrité des données : (vous devriez obtenir 943 users et 1681 items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comptez le nombre d'user et de films\n",
    "n_users = \n",
    "n_items =\n",
    "print (str(n_users) + ' users')\n",
    "print (str(n_items) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etudions les moyennes de chaque film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creez un dataframe 'films' contenant la moyenne des rating pour chaque film [pd.groupby] \n",
    "films =\n",
    "\n",
    "print(films.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons le nombre de ratings par film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rajoutez une colonne 'number_of_ratings' au dataframe films\n",
    "films = \n",
    "\n",
    "films.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons maintenant la distribution des avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'films' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7f9c6522a089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# affichez un histogramme de la colonne 'rating' [pls.hist()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfilms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'films' is not defined"
     ]
    }
   ],
   "source": [
    "# affichez un histogramme de la colonne 'rating' [pls.hist()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis la ponderation qui lui est associée..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichez un histogramme de la colonne 'number_of_ratings' [pls.hist()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et fusionnons les deux !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creez un graphique en x rating et en y number_of_ratings [sns.joinplot]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous pouvons le voir la majorité des films n'ont que peu de rating.<br>\n",
    "Notre dataset peut etre biaisé par ce defaut. Nous allons tout de meme continuer, et voir si les resultats sont coherents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mise en place du collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decalons notre liste pour l'aligner sur l'index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in data.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons maintenant l'indice de parcimonie (indiquant quel pourcentage des films chaque utilisateur a evalué en moyenne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisons a la taille totale de l'echantillon (le nombre de valeurs differentes de 0)\n",
    "parcimonie =\n",
    "# divisons par la taille totale de la matrice\n",
    "parcimonie /=\n",
    "# multiplions par 100 pour ramener en %\n",
    "parcimonie *=\n",
    "\n",
    "print ('Parcimonie: {:4.2f}%'.format(parcimonie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisons notre dataset en deux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    # Une matrice de 0 de la meme forme et taille que ratings\n",
    "    test =\n",
    "    # une copie de ratings\n",
    "    train =\n",
    "    # iterer sur le nombre de lignes de la matrice\n",
    "    for in :\n",
    "        # selectionnons des nombres au hasard\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], size=10, replace=False)\n",
    "        #initialiser la case de train a 0.0\n",
    "        \n",
    "        #initialiser la case de test a la valeur correspondante dans ratings\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons une fonction calculant la similarité cosinus, <br>\n",
    "elle est tres lente si faite en boucle for, utilisez les fonctions numpy dot et diagonal.<br>\n",
    "Elle devra pouvoir s'appliquer soit aux users soit aux items ! [np.translate ; np.diagonal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon est la pour eviter les divisions par 0\n",
    "def cosine_similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # pour avoir la matrice des item il suffit de faire une translation de ratings.\n",
    "    # Pour kind=item et kind=user il faut initialiser sim au produit matriciel de lui meme et de sa translation\n",
    "    # Ne pas oublier d'ajouter epsilon a chaque case !\n",
    "    if kind = 'user':\n",
    "        sim =\n",
    "    if kind = 'item':\n",
    "        sim =\n",
    "    # la racine carré de la diagonale de sim\n",
    "    norms =\n",
    "    # retournez sim divisé par norms divisé par la translation de norms. \n",
    "    return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = cosine_similarity(train, kind='user')\n",
    "item_similarity = cosine_similarity(train, kind='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definissons une fonction de prediction basique, qui prendra parti des map de similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings, similarity, kind='user'):\n",
    "    if kind == 'user':\n",
    "        return similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif kind == 'item':\n",
    "        # Indice : le produit matriciel est inversé et l'array de la somme n'est pas inverseé \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et une dernière pour calculer la fonction de cout \"mean squared error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(pred, actual):\n",
    "    # calculer la moyenne du carré des distance entre les predictions et les vraies valeurs\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on lance le tout !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prediction = predict(train, item_similarity, kind='item')\n",
    "user_prediction = predict(train, user_similarity, kind='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-875e3592676c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'User-based cost : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Item-based cost : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "print ('User-based cost : ' + get_mse(user_prediction, test))\n",
    "print ('Item-based cost : ' + get_mse(item_prediction, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vos valeurs doivent s'approcher de 8.3 pour le user-based et de 11.5 pour le item-based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation :\n",
    "\n",
    "Comme nous l'avons vus sur le graph, les datas souffent de beaucoup de biais qu'il faut prendre en compte avant leur traitement<br>\n",
    "Utilisons la methode top-k, qui consiste betement a ne prendre en compte que les k utilisateur les plus proches du notre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk(ratings, similarity, kind='user', k=40):\n",
    "    pred = np.zeros(ratings.shape)\n",
    "    if kind == 'user':\n",
    "        for i in range(ratings.shape[0]):\n",
    "            top_k_users = [np.argsort(similarity[:,i])[:-k-1:-1]]\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similarity[i, :][top_k_users].dot(ratings[:, j][top_k_users]) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[i, :][top_k_users]))\n",
    "    if kind == 'item':\n",
    "        for j in range(ratings.shape[1]):\n",
    "            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]\n",
    "            for i in range(ratings.shape[0]):\n",
    "                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items]))        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous allez le voir, le cout est maintenant divisé par deux !<br>\n",
    "Ce genre d'optimisation est rapide a mettre en place, et tres efficace.<br>\n",
    "Vous devriez obtenir autour de 6.5 pour les user-based et 7.8 pour les item-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_topk(train, user_similarity, kind='user', k=30)\n",
    "print ('Top-k User-based CF MSE: ' + str(get_mse(pred, test)))\n",
    "\n",
    "pred = predict_topk(train, item_similarity, kind='item', k=15)\n",
    "print ('Top-k Item-based CF MSE: ' + str(get_mse(pred, test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters customisation :\n",
    "Nous allons maintenant tracer un graphique du cout en fonction des k-premiers utilisateurs, et voir comment potentiellement ameliorer les performances de l'algorithme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_array = [5, 15, 30, 50, 100]\n",
    "user_train_mse = []\n",
    "user_test_mse = []\n",
    "item_test_mse = []\n",
    "item_train_mse = []\n",
    "\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)\n",
    "\n",
    "for k in k_array:\n",
    "    print(k)\n",
    "    user_pred = predict_topk(train, user_similarity, kind='user', k=k)\n",
    "    item_pred = predict_topk(train, item_similarity, kind='item', k=k)\n",
    "    \n",
    "    user_train_mse += [get_mse(user_pred, train)]\n",
    "    user_test_mse += [get_mse(user_pred, test)]\n",
    "    \n",
    "    item_train_mse += [get_mse(item_pred, train)]\n",
    "    item_test_mse += [get_mse(item_pred, test)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "pal = sns.color_palette(\"Set2\", 2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(k_array, user_train_mse, c=pal[0], label='User-based train', alpha=0.5, linewidth=5)\n",
    "plt.plot(k_array, user_test_mse, c=pal[0], label='User-based test', linewidth=5)\n",
    "plt.plot(k_array, item_train_mse, c=pal[1], label='Item-based train', alpha=0.5, linewidth=5)\n",
    "plt.plot(k_array, item_test_mse, c=pal[1], label='Item-based test', linewidth=5)\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.xticks(fontsize=16);\n",
    "plt.yticks(fontsize=16);\n",
    "plt.xlabel('k', fontsize=30);\n",
    "plt.ylabel('MSE', fontsize=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce graph represente le cout des differents set de data en fonction du nombre d'users pris en compte.<br>\n",
    "Il va nous permettre de determiner quels sont les k ideaux !<br>\n",
    "Nous pouvons voir ici que les valeurs les plus optimales sont autour de 40 pour le user-based test, et de 20 pour l'item-based test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Maintenant ajoutons les noms de film pour voir le resultat de notre travail !\n",
    "Vous allez devoir ouvrir le fichier u.item, y lire les index de chaque film pour les rentrer manuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_index = {}\n",
    "with open('ml-100k/u.item', 'r', encoding = 'ISO-8859-1') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split('|')\n",
    "        movies_index[int(info[0])-1] = info[1]\n",
    "        \n",
    "def top_movies(similarity, mapper, movie_idx, k=6):\n",
    "    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]\n",
    "top_k_movies(item_similarity, movies_index, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "Essayez de mettre en place une fonction predict qui normalise les données des users, <br>\n",
    "en prenant en compte leur note max et leur note min pour les ramener a une echelle commune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nobias(ratings, similarity, kind='user'):\n",
    "    if kind == 'user':\n",
    "        user_bias = ratings.mean(axis=1)\n",
    "        ratings = (ratings - user_bias[:, np.newaxis]).copy()\n",
    "        pred = similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "        pred += user_bias[:, np.newaxis]\n",
    "    elif kind == 'item':\n",
    "        item_bias = ratings.mean(axis=0)\n",
    "        ratings = (ratings - item_bias[np.newaxis, :]).copy()\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "        pred += item_bias[np.newaxis, :]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pred = predict_nobias(train, user_similarity, kind='user')\n",
    "print ('Bias-subtracted User-based : ' + str(get_mse(user_pred, test)))\n",
    "\n",
    "item_pred = predict_nobias(train, item_similarity, kind='item')\n",
    "print ('Bias-subtracted Item-based : ' + str(get_mse(item_pred, test)))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
